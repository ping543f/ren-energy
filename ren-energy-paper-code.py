# -*- coding: utf-8 -*-
"""renewable energy demand forecasting.ipynb

Automatically generated by Colaboratory.

"""

!pip install plot_keras_history

import numpy as np
import matplotlib.pyplot as plt

# Generate sample data
epochs = np.arange(1, 11)
training_accuracy = np.random.rand(10)
loss = np.random.rand(10)

# Create graph
plt.plot(epochs, training_accuracy, 'bo-', label='Training Accuracy')
plt.plot(epochs, loss, 'ro-', label='Loss')
plt.xlabel('Epochs')
plt.ylabel('Accuracy/Loss')
plt.legend()
plt.show()



import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression
# Generate sample data
x_train = np.random.rand(1000, 1)
y_train = np.sin(2*np.pi*x_train) + np.random.normal(0, 0.1, (1000, 1))


X, y = make_regression(n_samples=100, n_features=2, noise=0.1)
regr = RandomForestRegressor(max_depth=2, random_state=0)
regr.fit(X, y)

score = regr.score(X,y)
score

from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(x_train,y_train)
mae

import os
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import sklearn.preprocessing
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import confusion_matrix
# from sklearn.metrics import plot_confusion_matrix
from plot_keras_history import plot_history
from keras.layers import Dense,Dropout,SimpleRNN,LSTM
from keras.models import Sequential
from google.colab import files
from google.colab import drive
drive.mount('/content/drive')



df = pd.read_csv("/content/drive/MyDrive/renewable energy/PJME_hourly.csv",index_col='Datetime', parse_dates=['Datetime'])
df.head()

df.plot(figsize=(16,4),legend=True)

plt.title('AEP hourly energy demand - Without Normalization')

plt.show()

scaler = sklearn.preprocessing.MinMaxScaler()
df['PJME_MW']=scaler.fit_transform(df['PJME_MW'].values.reshape(-1,1))
df.shape

df.plot(figsize=(16,4),legend=True)

plt.title('AEP hourly energy demand - With Normalization')

plt.show()

def load_data(stock, seq_len):
    X_train = []
    y_train = []
    for i in range(seq_len, len(stock)):
        X_train.append(stock.iloc[i-seq_len : i, 0])
        y_train.append(stock.iloc[i, 0])

    #1 last 6189 days are going to be used in test
    X_test = X_train[100000:]
    y_test = y_train[100000:]

    #2 first 110000 days are going to be used in training
    X_train = X_train[:100000]
    y_train = y_train[:100000]

    #3 convert to numpy array
    X_train = np.array(X_train)
    y_train = np.array(y_train)

    X_test = np.array(X_test)
    y_test = np.array(y_test)

    #4 reshape data to input into RNN models
    X_train = np.reshape(X_train, (100000, seq_len, 1))
    X_test = np.reshape(X_test, (X_test.shape[0], seq_len, 1))

    return [X_train, y_train, X_test, y_test]

seq_len = 20 #choose sequence length

X_train, y_train, X_test, y_test = load_data(df, seq_len)

print('X_train.shape = ',X_train.shape)
print('y_train.shape = ', y_train.shape)
print('X_test.shape = ', X_test.shape)
print('y_test.shape = ',y_test.shape)

from keras.utils.vis_utils import plot_model
lstm_model = Sequential()

lstm_model.add(LSTM(200,activation="tanh",return_sequences=True, input_shape=(X_train.shape[1],1)))
lstm_model.add(Dropout(0.1))

lstm_model.add(LSTM(200,activation="tanh",return_sequences=False))
lstm_model.add(Dropout(0.1))

# lstm_model.add(LSTM(40,activation="tanh",return_sequences=False))
# lstm_model.add(Dropout(0.15))

lstm_model.add(Dense(1))

lstm_model.summary()

dot_img_file = '/tmp/model_1.png'
tf.keras.utils.plot_model(lstm_model, to_file=dot_img_file, show_shapes=True)

loss_functions = ['mean_squared_error', 'mean_absolute_error']
lstm_model.compile(optimizer="adam",loss=loss_functions)
history = lstm_model.fit(X_train, y_train, epochs=10, batch_size=3000, validation_data=(X_test, y_test))
lstm_model.save("lstm_energy_PJME.h5")

import matplotlib.pyplot as plt

# Get training and validation loss
loss = history.history['loss']
val_loss = history.history['val_loss']

# Plot training and validation loss
plt.plot(val_loss, label='Train Loss', marker="X")
plt.plot(loss, label='Test Loss', marker="*")

plt.title('Train and Test Loss for PJME dataset')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.grid()
plt.legend()
plt.show()

from keras.layers import GRU

gru_model = Sequential()

gru_model.add(GRU(200, activation="tanh", return_sequences=True, input_shape=(X_train.shape[1], 1)))
gru_model.add(Dropout(0.1))

gru_model.add(GRU(200, activation="tanh", return_sequences=False))
gru_model.add(Dropout(0.1))

gru_model.add(Dense(1))

gru_model.summary()

dot_img_file = '/tmp/model_1.png'
tf.keras.utils.plot_model(gru_model, to_file=dot_img_file, show_shapes=True)
gru_model.compile(optimizer="adam", loss="MSE")
history = gru_model.fit(X_train, y_train, epochs=10, batch_size=1000, validation_data=(X_test, y_test))
gru_model.save("gru_energy_pjme.h5")


# Get training and validation loss
loss = history.history['loss']
val_loss = history.history['val_loss']

# Plot training and validation loss
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# files.download("/content/lstm_energy_pjme.h5")

lstm_predictions = lstm_model.predict(X_test)

lstm_score_r2 = r2_score(y_test, lstm_predictions)
lstm_score_mae = mean_absolute_error(y_test, lstm_predictions)
lstm_score_mse = mean_squared_error(y_test, lstm_predictions)
# lstm_score_cf = confusion_matrix(y_test, lstm_predictions)

print("R^2 Score of LSTM model = ",lstm_score_r2)
print("MAE Score of LSTM model = ",lstm_score_mae)
print("MSE Score of LSTM model = ",lstm_score_mse)
# print("Confusion matrix of LSTM model = ",lstm_score_cf)

gru_predictions = gru_model.predict(X_test)

gru_score_r2 = r2_score(y_test, gru_predictions)
gru_score_mae = mean_absolute_error(y_test, gru_predictions)
gru_score_mse = mean_squared_error(y_test, gru_predictions)
# lstm_score_cf = confusion_matrix(y_test, lstm_predictions)

print("R^2 Score of GRU model = ",gru_score_r2)
print("MAE Score of GRU model = ",gru_score_mae)
print("MSE Score of GRU model = ",gru_score_mse)
# print("Confusion matrix of LSTM model = ",lstm_score_cf)

def plot_predictions(test, predicted, title):
    plt.figure(figsize=(25,10))
    plt.plot(test, color='green',label='Actual energy demand',linewidth=1)
    plt.plot(predicted, alpha=.6, color='red',label='Predicted energy demand',linewidth=1)
    plt.title(title)
    plt.xlabel('Hourly data points',fontsize=14)
    plt.ylabel('Normalized power consumption scale',fontsize=14)
    plt.legend(fontsize=14)
    plt.grid(color = 'grey', linestyle = '--', linewidth = 0.75, axis='both')
    # plt.colormaps="viridis"
    plt.show()


plot_predictions(y_test, lstm_predictions, "")



plot_history(history, path="interpolated.png", interpolate=True)
plt.close()

"""SARIMA"""

!pip install pmdarima
!pip install prophet



# Commented out IPython magic to ensure Python compatibility.
from prophet import Prophet
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import warnings
import itertools
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller
import statsmodels.api as sm
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
warnings.filterwarnings("ignore")
plt.style.use('fivethirtyeight')

df = pd.read_csv("/content/drive/MyDrive/PJME_hourly.csv",index_col='Datetime', parse_dates=['Datetime'])
df.head()

#Convert 'DATE' object to date datatype
# df['Datetime'] = pd.to_datetime(df['Datetime'])

#Visualize the dataframe
# plt.figure(figsize=(10,5))
# sns.lineplot(data=df, x="Datetime", y="PJME_MW")
# plt.title("Hourly energy demand")
# plt.grid(True)
# plt.show()

p = d = q = range(0, 2)

# Generate all different combinations of p, q and q triplets
simple_pdq = list(itertools.product(p, d, q))

# Generate all different combinations of seasonal p, q and q triplets
seasonal_pdq = [(i[0], i[1], i[2], 24) for i in list(itertools.product(p, d, q))]

print('Parameter combinations for Seasonal ARIMA...')

warnings.filterwarnings("ignore") # specify to ignore warning messages

for param in simple_pdq:
    for param_seasonal in seasonal_pdq:
        try:
            mod = sm.tsa.statespace.SARIMAX(df['PJME_MW'],
                                            order=param,
                                            )

            results = mod.fit()

            print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))
        except:
            continue

best_model = SARIMAX(df['PJME_MW'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 24)).fit()
print(best_model.summary())

#Forecasting 10 years ahead
forecast_values = best_model.get_forecast(steps = 120)

#Confidence intervals of the forecasted values
forecast_ci = forecast_values.conf_int()

#Plot the data
ax = df.plot(x='Datetime' ,y='PJME_MW', figsize = (12, 5), legend = True)

#Plot the forecasted values
forecast_values.predicted_mean.plot(ax=ax, label='Forecasts', figsize = (12, 5), grid=True)

#Plot the confidence intervals
ax.fill_between(forecast_ci.index,
                forecast_ci.iloc[: , 0],
                forecast_ci.iloc[: , 1], color='#D3D3D3', alpha = .5)
plt.title("Hourly energy demand", size=16)
plt.ylabel('PJME_MW', size=12)
plt.xlabel('Date time', size=12)
plt.legend(loc='upper center', prop={'size': 12})
#annotation
ax.text(1235, 82, 'FORECAST', fontsize=11,  color='RED')
ax.text(1275, 72, 'TO', fontsize=11,  color='RED')
ax.text(1260, 62, '2030', fontsize=11,  color='RED')
plt.show()

#divide into train and validation set to calculate R-squared score and mean absolute percentage error
train = df[:int(0.85*(len(df)))]
test = df[int(0.85*(len(df))):]
start=len(train)
end=len(train)+len(test)-1
predictions = best_model.predict(start=start, end=end, dynamic=False, typ='levels').rename('SARIMA Predictions')
evaluation_results = pd.DataFrame({'r2_score': r2_score(test['PJME_MW'], predictions)}, index=[0])
evaluation_results['mean_absolute_error'] = mean_absolute_error(test['PJME_MW'], predictions)
evaluation_results['root_mean_squared_error'] = np.sqrt(mean_squared_error(test['PJME_MW'], predictions))
evaluation_results

df['sarima_model'] = best_model.fittedvalues
forecast = best_model.predict(start=df.shape[0], end=df.shape[0] + 120)
forecast = df['sarima_model'].append(forecast)
plt.figure(figsize=(12, 5))
plt.plot(forecast, color='r', label='Forecast')
plt.axvspan(df.index[-1], forecast.index[-1], alpha=0.6, color='lightgrey')
plt.plot(df['PJME_MW'], label='PJME_MW')
plt.legend()
plt.show()

"""PROPHET"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
from prophet import Prophet
import plotly.express as px
import plotly.offline as py
py.init_notebook_mode()
# %matplotlib inline

df = pd.read_csv("/content/drive/MyDrive/AEP_hourly.csv")
df.head()

#Convert 'DATE' object to date datatype
df['Datetime'] = pd.to_datetime(df['Datetime'])


fig = px.line(df, x="Datetime", y="AEP_MW", title='Hourly energy demand',markers=None)
fig.update_xaxes(rangeslider_visible=True)
fig.show(renderer="colab")

df = df.rename(columns={'Datetime':'ds', 'AEP_MW':'y'})

# plt.rcParams['figure.figsize']=(20,10)
# plt.style.use('ggplot')
# pd.plotting.register_matplotlib_converters()
# df.set_index('ds').y.plot(marker='X')
# df['y'] = np.log(df['y'])
# df.head()
# df.set_index('ds').y.plot(marker="X").get_figure()

model = Prophet(weekly_seasonality=False,daily_seasonality=False,yearly_seasonality=False,interval_width=0.95)
model.fit(df)

future = model.make_future_dataframe(periods=6,freq='h')
forecast = model.predict(future)

forecast.tail()

from sklearn.model_selection import train_test_split

train_data = df.sample(frac=0.8, random_state=10)
validation_data = df.drop(train_data.index)

print(f'training data size : {train_data.shape}')
print(f'validation data size : {validation_data.shape}')

train_data = train_data.reset_index()
validation_data = validation_data.reset_index()

prediction = model.predict(pd.DataFrame({'ds':validation_data['ds']}))
y_actual = validation_data['y']
y_predicted = prediction['yhat']
y_predicted = y_predicted.astype(int)
mae = mean_absolute_error(y_actual, y_predicted)
mse = mean_squared_error(y_actual, y_predicted)
r2 = r2_score(y_actual, y_predicted)
print(f"Prophet MAE: {mae} \n Prophet MSE: {mse} \n Prophet R2: {r2}")

from prophet.diagnostics import cross_validation
df_cv = cross_validation(model, initial='72 hours', period='1 hour', horizon = '24 hours')
df_cv.head()

from prophet.diagnostics import performance_metrics
df_p = performance_metrics(df_cv)
df_p.head()

from prophet.plot import add_changepoints_to_plot
# plt.figure(figsize=(30, 30))
# ax.figure.se
fig = model.plot(forecast,figsize=(15,10))
# a = add_changepoints_to_plot(fig.gca(), model, forecast)
ax = fig.gca()
ax.legend(["Actual",'Prophet predicted'])
ax.set_xlabel("Date time")
ax.set_ylabel("Energy demand")

"""SVR"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
# %matplotlib inline
import seaborn as sns
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error,r2_score,mean_squared_error,mean_absolute_percentage_error
import numpy as np
from google.colab import files
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/AEP_hourly.csv")
df.head()

#Convert 'DATE' object to date datatype
df['Datetime'] = pd.to_datetime(df['Datetime'])

df.info()
df.head()
df.describe()

df['hourly_data'] = [i for i in range(1,len(df['Datetime'])+1)]
df.tail()

# Creating Train And Target Data
# train Data
hourly_data = df['hourly_data'].values.reshape(len(df['hourly_data']),1)
# Target Data
target = df['AEP_MW']

svr = SVR(kernel="rbf",C = 1e3,gamma = 0.1)
svr.fit(hourly_data,target)
pred = svr.predict(hourly_data)

f_hour = [[121274],[121275],[121276],[121277],[121278]]
future = svr.predict(f_hour)

print("R^2 : ", r2_score(target, pred))
print("MAE :", mean_absolute_error(target,pred))
print("MSE:", mean_squared_error(target,pred))
print("RMSE:", np.sqrt(mean_squared_error(target,pred)))
print("MAPE:", mean_absolute_percentage_error(target,pred))

# print(week)
# print(svr.predict([[52],[53],[54],[55]]))
# plotting prediction and real values
plt.figure(figsize=(15,5))
plt.scatter(hourly_data,target,label ='Actual energy demand',color="red")
plt.plot(hourly_data,pred,'b',marker='*',label ='Predicted energy demand')
plt.plot(f_hour,svr.predict(f_hour),'g',marker='o',label ='Futre Predicted energy demand')
plt.xlabel("Hourly data")
plt.ylabel("Energy demand")
plt.title("SVR- Actual vs predicted data")
plt.legend()

plt.figure(figsize=(15,5))

X= hourly_data
y= target
svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)
svr_lin = SVR(kernel='linear', C=1e3)
svr_poly = SVR(kernel='poly', C=1e3, degree=2)
y_rbf = svr_rbf.fit(X, y).predict(X)
y_lin = svr_lin.fit(X, y).predict(X)
y_poly = svr_poly.fit(X, y).predict(X)

lw = 2
plt.scatter(X, y, color='red', label='Actual energy demand')
# plt.hold('on')
plt.plot(X, y_rbf, color='navy', lw=lw, label='RBF model')
plt.plot(X, y_lin, color='c', lw=lw, label='Linear model')
plt.plot(X, y_poly, color='cornflowerblue', lw=lw, label='Polynomial model')
plt.xlabel('Hourly data')
plt.ylabel('Energy demand')
plt.title('Different kernels of SVR')
plt.legend()
plt.show()